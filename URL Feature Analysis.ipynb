{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b3a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib, bs4, re\n",
    "import googlesearch\n",
    "import whois\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import phishtank\n",
    "import socket\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import favicon\n",
    "import xml.etree.ElementTree as ET \n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "from subprocess import *\n",
    "import json\n",
    "import base64\n",
    "\n",
    "#if url contains ip addresses instead of name\n",
    "def have_ip_address(url):\n",
    "    match=re.search('(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  #IPv4\n",
    "                    '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)'  #IPv4 in hexadecimal\n",
    "                    '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}',url)     #Ipv6\n",
    "    if match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "#length of urls\n",
    "def url_length(url):\n",
    "    if len(url)<54:\n",
    "        return 1\n",
    "    elif len(url)>=54|len(url)<=75:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "#if url contains shortening services\n",
    "def url_shortener(url):\n",
    "    match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                    'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
    "\n",
    "    if match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "#url having @ symbol\n",
    "def have_atrate_symbol(url):\n",
    "    match = re.search('@',url)\n",
    "\n",
    "    if match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "#double slash redirecting\n",
    "def double_slash_redirect(url):\n",
    "    list = [x.start(0) for x in re.finditer('\\\\.',url)]\n",
    "    if list[len(list)-1]>6:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "#having hyphen in urls\n",
    "def prefix_suffix(url):\n",
    "    match = re.search('-',url)\n",
    "    if match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "#finding subdomains in a domain\n",
    "def have_subdomain(url):\n",
    "    if(have_ip_address(url)==-1):\n",
    "        match = re.search('(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5]))|(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}',url)\n",
    "        pos = match.end(0)\n",
    "        url = url[pos:]\n",
    "    list = [x.start(0) for x in re.finditer('\\.',url)]\n",
    "    if len(list)<=3:\n",
    "        return 1\n",
    "    elif len(list) == 4:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "#SSHFinal\n",
    "\n",
    "#Domain_registration_length\n",
    "def domain_registration_length(domain):\n",
    "    expiry_date = domain.expiration_date\n",
    "    exp = datetime.strftime(expiry_date,\"%Y-%m-%d\")\n",
    "    expires = datetime.strptime(exp,\"%Y-%m-%d\")\n",
    "    today = datetime.today()\n",
    "    tp = datetime.strftime(today,\"%Y-%m-%d\")\n",
    "    today_date = datetime.strptime(tp,\"%Y-%m-%d\")\n",
    "    registration_length = abs((expires - today_date).days)\n",
    "\n",
    "    if registration_length / 365 <= 1:\n",
    "    \treturn -1\n",
    "    else:\n",
    "    \treturn 1\n",
    "\n",
    "\n",
    "#website has favicon\n",
    "\n",
    "def favicon(wiki,soup,domain):\n",
    "    for head in soup.find_all('link'):\n",
    "        for head.link in soup.find_all('link',href=True):\n",
    "            dots = [x.start(0) for x in re.finditer('\\.', head.link['href'])]\n",
    "            if wiki in head.link['href'] or len(dots) == 1 or domain in head.link['href']:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "    return 1\n",
    "\n",
    "#Checking Ports\n",
    "status_port = []\n",
    "import socket\n",
    "def isOpen(url,port_numbers):\n",
    "    for port in port_numbers:\n",
    "        ip = socket.gethostbyname(url)\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        try:\n",
    "            s.connect((ip, port))\n",
    "            s.shutdown(2)\n",
    "            status_port.append(0)\n",
    "        except:\n",
    "            status_port.append(1)\n",
    "    if(status_port[3] == 0 & status_port[4] == 0 & status_port[0] == 1 & status_port[1] == 1 & status_port[2] == 1):\n",
    "    \treturn -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "#http tokens\n",
    "def https_token(url):\n",
    "    match = [(x.start(0), x.end(0)) for x in re.finditer('https:// | http:// | http | https', url)]\n",
    "\n",
    "    if len(match)!= 1:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "#Request URLs\n",
    "def request_url(wiki, soup, domain):\n",
    "    i = 0\n",
    "    success = 0\n",
    "    for img in soup.find_all('img',src=True):\n",
    "        dots = [x.start(0) for x in re.finditer('\\.',img['src'])]\n",
    "        if wiki in img['src'] or domain in img['src'] or len(dots)==1:\n",
    "         success = success + 1\n",
    "        i=i+1\n",
    "\n",
    "    for audio in soup.find_all('audio', src= True):\n",
    "      dots = [x.start(0) for x in re.finditer('\\.', audio['src'])]\n",
    "      if wiki in audio['src'] or domain in audio['src'] or len(dots)==1:\n",
    "         success = success + 1\n",
    "      i=i+1\n",
    "\n",
    "    for embed in soup.find_all('embed', src= True):\n",
    "      dots=[x.start(0) for x in re.finditer('\\.',embed['src'])]\n",
    "      if wiki in embed['src'] or domain in embed['src'] or len(dots)==1:\n",
    "         success = success + 1\n",
    "      i=i+1\n",
    "\n",
    "    for iframe in soup.find_all('iframe', src= True):\n",
    "      dots=[x.start(0) for x in re.finditer('\\.',iframe['src'])]\n",
    "      if wiki in iframe['src'] or domain in iframe['src'] or len(dots)==1:\n",
    "         success = success + 1\n",
    "      i=i+1\n",
    "\n",
    "    try:\n",
    "       percentage = success/float(i) * 100\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "    if percentage < 22.0 :\n",
    "       return 1\n",
    "    elif((percentage >= 22.0) and (percentage < 61.0)) :\n",
    "       return 0\n",
    "    else :\n",
    "       return -1\n",
    "\n",
    "\n",
    "#url anchor tags\n",
    "def url_of_anchor(wiki, soup, domain):\n",
    "    i = 0\n",
    "    unsafe=0\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if \"#\" in a['href'] or \"javascript\" in a['href'].lower() or \"mailto\" in a['href'].lower() or not (wiki in a['href'] or domain in a['href']):\n",
    "            unsafe = unsafe + 1\n",
    "        i = i + 1\n",
    "    try:\n",
    "        percentage = unsafe / float(i) * 100\n",
    "    except:\n",
    "        return 1\n",
    "    if percentage < 31.0:\n",
    "        return 1\n",
    "    elif ((percentage >= 31.0) and (percentage < 67.0)):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# Links in <Script> and <Link> tags\n",
    "def links_in_tags(wiki, soup, domain):\n",
    "   i=0\n",
    "   success =0\n",
    "   for link in soup.find_all('link', href= True):\n",
    "      dots=[x.start(0) for x in re.finditer('\\.',link['href'])]\n",
    "      if wiki in link['href'] or domain in link['href'] or len(dots)==1:\n",
    "         success = success + 1\n",
    "      i=i+1\n",
    "\n",
    "   for script in soup.find_all('script', src= True):\n",
    "      dots=[x.start(0) for x in re.finditer('\\.',script['src'])]\n",
    "      if wiki in script['src'] or domain in script['src'] or len(dots)==1 :\n",
    "         success = success + 1\n",
    "      i=i+1\n",
    "   try:\n",
    "       percentage = success / float(i) * 100\n",
    "   except:\n",
    "       return 1\n",
    "\n",
    "   if percentage < 17.0 :\n",
    "      return 1\n",
    "   elif((percentage >= 17.0) and (percentage < 81.0)) :\n",
    "      return 0\n",
    "   else :\n",
    "      return -1\n",
    "\n",
    "\n",
    "# Server Form Handler (SFH)\n",
    "###### Have written consitions directly from word file..as there are no sites to test ######\n",
    "def sfh(wiki, soup, domain):\n",
    "   for form in soup.find_all('form', action= True):\n",
    "      if form['action'] ==\"\" or form['action'] == \"about:blank\" :\n",
    "         return -1\n",
    "      elif wiki not in form['action'] and domain not in form['action']:\n",
    "          return 0\n",
    "      else:\n",
    "            return 1\n",
    "   return 1\n",
    "\n",
    "#Mail Function\n",
    "###### PHP mail() function is difficult to retreive, hence the following function is based on mailto ######\n",
    "def submitting_to_email(soup):\n",
    "   for form in soup.find_all('form', action= True):\n",
    "      if \"mailto:\" in form['action'] :\n",
    "         return -1\n",
    "      else:\n",
    "          return 1\n",
    "   return 1\n",
    "\n",
    "\n",
    "\n",
    "#Abnormal URLs\n",
    "def abnormal_url(domain,url):\n",
    "    hostname=domain.name\n",
    "    match=re.search(hostname,url)\n",
    "    if match:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def url_validator(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc, result.path])\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def redirect(url):\n",
    "    count=0\n",
    "    if count<=1:\n",
    "        return 1\n",
    "    elif count>=2 and count<4:\n",
    "        return  0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "    \n",
    "def on_mouseover(url):\n",
    "  try:\n",
    "    html_content = requests.get(url).text\n",
    "  except:\n",
    "    return -1\n",
    "  soup = BeautifulSoup(html_content, \"lxml\")\n",
    "  if str(soup).lower().find('onmouseover=\"window.status') != -1:\n",
    "    return -1\n",
    "  return 1\n",
    "\n",
    "def rightClick(url):\n",
    "\n",
    "  return 1\n",
    "\n",
    "\n",
    "def popup(url):\n",
    "\n",
    "        return 1\n",
    "\n",
    "def page_rank(url):\n",
    "\n",
    "  return 1\n",
    "\n",
    "\n",
    "def links_pointing(url):\n",
    "\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "def SSLfinal_State(url):\n",
    "    try:\n",
    "#check wheather contains https       \n",
    "        if(regex.search('^https',url)):\n",
    "            usehttps = 1\n",
    "        else:\n",
    "            usehttps = 0\n",
    "#getting the certificate issuer to later compare with trusted issuer \n",
    "        #getting host name\n",
    "        subDomain, domain, suffix = extract(url)\n",
    "        host_name = domain + \".\" + suffix\n",
    "        context = ssl.create_default_context()\n",
    "        sct = context.wrap_socket(socket.socket(), server_hostname = host_name)\n",
    "        sct.connect((host_name, 443))\n",
    "        certificate = sct.getpeercert()\n",
    "        issuer = dict(x[0] for x in certificate['issuer'])\n",
    "        certificate_Auth = str(issuer['commonName'])\n",
    "        certificate_Auth = certificate_Auth.split()\n",
    "        if(certificate_Auth[0] == \"Network\" or certificate_Auth == \"Deutsche\"):\n",
    "            certificate_Auth = certificate_Auth[0] + \" \" + certificate_Auth[1]\n",
    "        else:\n",
    "            certificate_Auth = certificate_Auth[0] \n",
    "        trusted_Auth = ['Comodo','Symantec','GoDaddy','GlobalSign','DigiCert','StartCom','Entrust','Verizon','Trustwave','Unizeto','Buypass','QuoVadis','Deutsche Telekom','Network Solutions','SwissSign','IdenTrust','Secom','TWCA','GeoTrust','Thawte','Doster','VeriSign']        \n",
    "#getting age of certificate\n",
    "        startingDate = str(certificate['notBefore'])\n",
    "        endingDate = str(certificate['notAfter'])\n",
    "        startingYear = int(startingDate.split()[3])\n",
    "        endingYear = int(endingDate.split()[3])\n",
    "        Age_of_certificate = endingYear-startingYear\n",
    "        \n",
    "#checking final conditions\n",
    "        if((usehttps==1) and (certificate_Auth in trusted_Auth) and (Age_of_certificate>=1) ):\n",
    "            return -1 #legitimate\n",
    "        elif((usehttps==1) and (certificate_Auth not in trusted_Auth)):\n",
    "            return 0 #suspicious\n",
    "        else:\n",
    "            return 1 #phishing\n",
    "        \n",
    "    except Exception as e:\n",
    "        \n",
    "        return 1\n",
    "\n",
    "#IFrame Redirection\n",
    "###### Checking remaining on some site######\n",
    "def iframe(soup):\n",
    "    for iframe in soup.find_all('iframe', width=True, height=True, frameBorder=True):\n",
    "        if iframe['width']==\"0\" and iframe['height']==\"0\" and iframe['frameBorder']==\"0\":\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    return 1\n",
    "\n",
    "#Age of Domain\n",
    "def age_of_domain(domain):\n",
    "    # creation_date = domain.creation_date\n",
    "    # expiration_date = domain.expiration_date\n",
    "    # ageofdomain = abs((expiration_date - creation_date).days)\n",
    "    # if ageofdomain / 30 < 6:\n",
    "    #     return -1\n",
    "    # else:\n",
    "    #     return 1\n",
    "    creation_date = domain.creation_date\n",
    "    expiration_date = domain.expiration_date\n",
    "    create = datetime.strftime(creation_date,\"%Y-%m-%d\")\n",
    "    create_date = datetime.strptime(create,\"%Y-%m-%d\")\n",
    "    exp = datetime.strftime(expiration_date,\"%Y-%m-%d\")\n",
    "    exp_date = datetime.strptime(exp,\"%Y-%m-%d\")\n",
    "    domain_age = abs((exp_date - create_date).days)\n",
    "\n",
    "    if domain_age / 30 < 6:\n",
    "    \treturn -1\n",
    "    else:\n",
    "    \treturn 1\n",
    "\n",
    "#Traffic on website using Alexa\n",
    "def web_traffic(url):\n",
    "    try:\n",
    "        rank = bs4.BeautifulSoup(urllib.request(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK']\n",
    "    except TypeError:\n",
    "        return -1\n",
    "    rank= int(rank)\n",
    "    if (rank<100000):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "#Google Index\n",
    "def google_index(url):\n",
    "    site=googlesearch.search(url, 5)\n",
    "    if site:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def statistical_report(url,hostname):\n",
    "    url_match=re.search('esy\\.es | hol\\.es | \t000webhostapp\\.com | 16mb\\.com | bit\\.ly | for-our\\.info | beget\\.tech | blogspot\\.com | weebly\\.com |raymannag\\.ch',url)\n",
    "    try:\n",
    "        ip_address=socket.gethostbyname(hostname)\n",
    "    except:\n",
    "        print ('Connection problem. Please check your internet connection!')\n",
    "##### 1st line is phishtank top 10 domain ips and 2nd, 3rd, 4th, 5th, 6th lines are top 50 domain ips from stopbadware #####\n",
    "    ip_match=re.search('146\\.112\\.61\\.108 | 31\\.170\\.160\\.61 | 67\\.199\\.248\\.11 | 67\\.199\\.248\\.10 | 69\\.50\\.209\\.78 | 192\\.254\\.172\\.78 | \t216\\.58\\.193\\.65 | 23\\.234\\.229\\.68 | 173\\.212\\.223\\.160 | 60\\.249\\.179\\.122',ip_address)\n",
    "    if url_match:\n",
    "        return -1\n",
    "    elif ip_match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def main(url):\n",
    "    with open('markup.txt', 'r', encoding='utf-8') as file:\n",
    "        soup_string=file.read()\n",
    "\n",
    "    soup = BeautifulSoup(soup_string, 'html.parser')\n",
    "\n",
    "    status=[]\n",
    "\n",
    "    hostname = url\n",
    "    h = [(x.start(0), x.end(0)) for x in re.finditer('https://|http://|www.|https://www.|http://www.', hostname)]\n",
    "    z = int(len(h))\n",
    "    if z != 0:\n",
    "        y = h[0][1]\n",
    "        hostname = hostname[y:]\n",
    "        h = [(x.start(0), x.end(0)) for x in re.finditer('/', hostname)]\n",
    "        z = int(len(h))\n",
    "        if z != 0:\n",
    "            hostname = hostname[:h[0][0]]\n",
    "\n",
    "    status.append(have_ip_address(url))\n",
    "    status.append(url_length(url))\n",
    "    status.append(url_shortener(url))\n",
    "    status.append(have_atrate_symbol(url))\n",
    "    status.append(double_slash_redirect(url))\n",
    "    status.append(prefix_suffix(hostname))\n",
    "    status.append(have_subdomain(url))\n",
    "    status.append(SSLfinal_State(url))\n",
    "\n",
    "    dns=1\n",
    "    try:\n",
    "        domain = whois.query(hostname)\n",
    "    except:\n",
    "        dns=-1\n",
    "\n",
    "    if dns==-1:\n",
    "        status.append(-1)\n",
    "    else:\n",
    "        status.append(domain_registration_length(domain))\n",
    "\n",
    "    status.append(favicon(url,soup, hostname))\n",
    "\n",
    "    port_numbers = [21,22,23, 80,443]\n",
    "    status.append(isOpen(hostname,port_numbers))\n",
    "    status.append(https_token(url))\n",
    "    status.append(request_url(url, soup, hostname))\n",
    "    status.append(url_of_anchor(url, soup, hostname))\n",
    "    status.append(links_in_tags(url,soup, hostname))\n",
    "    status.append(sfh(url,soup, hostname))\n",
    "    status.append(submitting_to_email(soup))\n",
    "\n",
    "    if dns == -1:\n",
    "        status.append(-1)\n",
    "    else:\n",
    "        status.append(abnormal_url(domain,url))\n",
    "\n",
    "    status.append(redirect(url))\n",
    "    status.append(on_mouseover(url))\n",
    "    status.append(rightClick(url))\n",
    "    status.append(popup(url))\n",
    "    status.append(iframe(soup))\n",
    "\n",
    "    if dns == -1:\n",
    "        status.append(-1)\n",
    "    else:\n",
    "        status.append(age_of_domain(domain))\n",
    "\n",
    "    status.append(dns)\n",
    "\n",
    "    status.append(web_traffic(soup))\n",
    "    status.append(page_rank(url))\n",
    "    status.append(google_index(url))\n",
    "    status.append(links_pointing(url))\n",
    "    status.append(statistical_report(url,hostname))\n",
    "    '''\n",
    "    print ('\\n1. Having IP address\\n2. URL Length\\n3. URL Shortening service\\n4. Having @ symbol\\n5. Having double slash\\n' \\\n",
    "          '6. Having dash symbol(Prefix Suffix)\\n7. Having multiple subdomains\\n8. Domain Registration Length\\n9. Favicon\\n' \\\n",
    "          '10. Ports \\n11. HTTP or HTTPS token in domain name\\n12. Request URL\\n13. URL of Anchor\\n14. Links in tags\\n' \\\n",
    "          '15. SFH\\n16. Submitting to email\\n17. Abnormal URL\\n18. IFrame\\n19. Age of Domain\\n20. DNS Record\\n21. Web Traffic\\n' \\\n",
    "          '22. Google Index\\n23. Statistical Reports\\n')'''\n",
    "    print (status)\n",
    "    return status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
